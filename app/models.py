"""Python Module with functionality handling loading the model and generating text-generation pipeline and generating a text response from a given prompt."""

import torch
from transformers import Pipeline, pipeline

system_prompt = """
Your name is FastAPI bot and you are ahelpful chtbot
responsible for teaching FastAPI to your users.
Always respond in markdown
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def load_text_model() -> Pipeline:
    """
    Initalises the model and moves it to GPU if available else CPU.

    Returns:
        Pipe: Instance of the text-generation pipeline. Tokenises text, initiates model
        and performs post process to convert back to human readable.
    """
    pipe = pipeline(
        "text-generation",
        model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        torch_dtype=torch.bfloat16,
        device=device,
    )

    return pipe


def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7) -> str:
    """
    Formats a user prompt into a chat template and generates a response
    using the provided Hugging Face pipeline.

    Args:
        pipe (Pipeline): The Hugging Face text-generation pipeline containing the model and tokenizer.
        prompt (str): The raw text input provided by the user.
        temperature (float, optional):  Controls the randomness of the output.
            Higher values (e.g., 1.0) make it more creative; lower (e.g., 0.2) make it more focused.
            Defaults to 0.7.

    Returns:
        str: The clean text response generated by the AI assistant, excluding the original prompt.
    """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt},
    ]

    prompt = pipe.tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    predictions = pipe(
        prompt,
        temperature=temperature,
        max_new_tokens=256,
        do_sample=True,
        top_k=50,
        top_p=0.95,
    )
    output = predictions[0]["generated_text"].split("</s>\n<|assistant|>\n")[-1]
    return output
